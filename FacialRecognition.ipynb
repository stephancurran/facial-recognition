{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97472ce-5578-4e57-9d8b-67e7444a1689",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091e12e4-d4f7-4be2-a6e9-d71bc86cc401",
   "metadata": {},
   "source": [
    "<p>\n",
    "    I was inspired by <a href=\"https://towardsdatascience.com/real-time-face-recognition-an-end-to-end-project-b738bb0f7348\">this</a> article to try my hand at facial recognition. This notebook contains an app in two parts. The first uses a webcam and face detection to take mug shots of the user which are used to train a recognizer. The second part also uses face detection, but now uses the trained recognizer to predict whether the detected faces are in the training set.\n",
    "</p>\n",
    "\n",
    "| ![s](register_demo.gif)<br/><center>Face detection</center> | ![f](recognition_demo.gif)<br/><center>Face detection with facial recognition</center> |\n",
    "| -- | -- |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2972f6-541b-4dce-9c6b-34735a8776e1",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "<p>This notebook was written with Python 3.8.0 and the following libraries. If you have difficulties running it with other version, try using a virtual environment and running this cell.</p>\n",
    "\n",
    "<p><strong>Note:</strong> The code depends on a local webcam, so this won't work in Google Colab or other web-based environments.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb079a-3d3c-4e32-92f1-cd1b114b1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install opencv-contrib-python==4.8.0.76 imutils==0.5.4 matplotlib==3.7.3 numpy==1.24.4 imageio==2.31.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6bce7-97a1-40eb-9613-b8a85b6f7adc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b32dd-b4bd-4df7-9470-28071b4e9330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from typing import Any, List, Tuple, Union\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from imutils.video import VideoStream\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03946ab7-bf98-4754-b980-0b646968d9b5",
   "metadata": {},
   "source": [
    "## File paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfefba41-f292-4e0b-939a-f63aa256aa09",
   "metadata": {},
   "source": [
    "Let's download haarcascade_frontalface_default.xml, used for face detection, if we haven't already, and set some file paths that we'll need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0bd6a-c5d7-4f00-9e49-6dee2179c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "CASCADE_PATH = os.path.join(os.getcwd(), \"haarcascade_frontalface_default.xml\")\n",
    "CASCADE_URL = \"https://raw.githubusercontent.com/kipr/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
    "\n",
    "if not os.path.exists(CASCADE_PATH):\n",
    "    !wget {CASCADE_URL}\n",
    "\n",
    "FACE_DATA = os.path.join(os.getcwd(), \"face_data.yml\")\n",
    "\n",
    "SAVE_PATH = os.path.join(os.getcwd(), \"user_pics\")\n",
    "\n",
    "PIC_FILE_TEMPLATE = \"user_{0}_pic_{1}.jpg\"\n",
    "PIC_FILE_RE = PIC_FILE_TEMPLATE.format(\"(\\d+)\", \"\\d+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214f5fa1-8593-4337-a740-d0f54012918d",
   "metadata": {},
   "source": [
    "## Saving Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c83be65-820d-4ef5-a106-fbf4302280c5",
   "metadata": {},
   "source": [
    "These methods cover the saving of the user pics and gif recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff0b403-a2ce-4d8b-88c6-63a6e1c88b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_user_id() -> int:\n",
    "    \"\"\"\n",
    "    Return the next available user ID (first is 1)\n",
    "    \"\"\"\n",
    "\n",
    "    next_id = 1\n",
    "    for file in sorted(os.listdir(SAVE_PATH)):\n",
    "        match = re.search(PIC_FILE_RE, file)\n",
    "        if match:\n",
    "            if int(match.groups()[0]) == next_id:\n",
    "                next_id += 1\n",
    "    return next_id\n",
    "\n",
    "\n",
    "def save_user_images(img_list: List[np.ndarray]) -> None:\n",
    "    \"\"\"\n",
    "    Save the given list of images to the save directory with a unique prefix\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure the save directory exists\n",
    "    if not os.path.exists(SAVE_PATH):\n",
    "        os.makedirs(SAVE_PATH)\n",
    "\n",
    "    # Save the mug shots with a unique user ID\n",
    "    next_id = get_next_user_id()\n",
    "    for i, img in enumerate(img_list):\n",
    "        cv2.imwrite(os.path.join(SAVE_PATH, PIC_FILE_TEMPLATE.format(next_id, i)), img)\n",
    "\n",
    "    # Display the captured mug shots (or 10 random ones if there are more than that)\n",
    "    max_displayed_shots = 10\n",
    "    if len(img_list) > max_displayed_shots:\n",
    "        img_list = random.sample(img_list, k=max_displayed_shots)\n",
    "    rows = 2\n",
    "    cols = int(len(img_list) / rows)\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    for i in range(0, rows * cols):\n",
    "        img = cv2.cvtColor(img_list[i], cv2.COLOR_BGR2RGB)  # Because CV2 captures in BGR, but plt uses RGB\n",
    "        fig.add_subplot(rows, cols, i + 1)\n",
    "        plt.tick_params(left=False, labelleft=False, bottom=False, labelbottom=False)\n",
    "        plt.imshow(img)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_gif(frames: List[np.ndarray],\n",
    "             name: str) -> None:\n",
    "    \"\"\"\n",
    "    Shrink and colour correct the frames before saving them as a gif\n",
    "    \"\"\"\n",
    "    \n",
    "    resized_frames = [imutils.resize(frame, width=300) for frame in frames]\n",
    "    corrected_frames = [cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) for frame in resized_frames]\n",
    "    imageio.mimwrite(name, corrected_frames, fps=30, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be7cd0-5073-48c1-b543-3e7dbf21f7bf",
   "metadata": {},
   "source": [
    "# Video Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d48e9c-c473-4476-8803-06772b63f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many mug shots to take\n",
    "MAX_SHOTS = 20\n",
    "\n",
    "# Random value in [0, 1) must be greater than this to capture mug shot\n",
    "CAPTURE_THRESHOLD = 0.9\n",
    "\n",
    "# Confidence must be below this to recognise user\n",
    "RECOGNITION_THRESHOLD = 50\n",
    "\n",
    "# How big to resize the frame\n",
    "FRAME_WIDTH = 600\n",
    "\n",
    "# Modes\n",
    "MODE_WAITING = 0\n",
    "MODE_LEFT_RIGHT = 1\n",
    "MODE_UP_DOWN = 2\n",
    "MODE_AUTHENTICATING = 3\n",
    "\n",
    "MESSAGES = [\"Press S when ready to begin\",\n",
    "            \"Move your head left and right\",\n",
    "            \"Move your head up and down\",\n",
    "            \"\"]\n",
    "\n",
    "def add_text(frame: np.ndarray,\n",
    "             message: str) -> None:\n",
    "    \"\"\"\n",
    "    Add the message at bottom centre of the frame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Text Options\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    scale = 1\n",
    "    thickness = 2\n",
    "    colour = (255, 255, 255)\n",
    "\n",
    "    # Find the bottom centre of the frame\n",
    "    text_size = cv2.getTextSize(message, font, scale, thickness)[0]\n",
    "    x_pos = int((frame.shape[1] - text_size[0]) / 2)\n",
    "    y_pos = frame.shape[0] - 20\n",
    "\n",
    "    # Add the text\n",
    "    cv2.putText(frame, message, (x_pos, y_pos), font, scale, colour, thickness)\n",
    "\n",
    "\n",
    "def capture_frame(stream: VideoStream,\n",
    "                  cascade: cv2.CascadeClassifier) -> (np.ndarray, np.ndarray, Tuple[np.ndarray]):\n",
    "    \"\"\"\n",
    "    Get the next frame from the stream, mirror it, resize it, and convert it to greyscale\n",
    "    Find the faces in the frame\n",
    "    Return frame, grayscale version, and bounding boxes of faces\n",
    "    \"\"\"\n",
    "\n",
    "    frame = cv2.flip(stream.read(), 1)\n",
    "    frame = imutils.resize(frame, width=FRAME_WIDTH)\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = cascade.detectMultiScale(\n",
    "        gray_frame,     \n",
    "        scaleFactor=1.2,\n",
    "        minNeighbors=5,     \n",
    "        minSize=(100, 100)\n",
    "    )\n",
    "\n",
    "    return frame, gray_frame, faces\n",
    "\n",
    "\n",
    "def get_mug_shot(frame: np.ndarray,\n",
    "                 gray_frame: np.ndarray,\n",
    "                 face: Tuple[int]) -> Union[np.array, None]:\n",
    "    \"\"\"\n",
    "    Draw a rectangle around the face in the frame.\n",
    "    With some probability, capture and return this mug shot\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the bounding box\n",
    "    x_pos, y_pos, width, height = face\n",
    "\n",
    "    # Draw a rectangle around the face.\n",
    "    frame = cv2.rectangle(frame, (x_pos, y_pos), (x_pos + width, y_pos + height), (255, 255, 255), 2)\n",
    "\n",
    "    # If the threshold is passed, capture the mug shot\n",
    "    if np.random.random() > CAPTURE_THRESHOLD:\n",
    "        return gray_frame[y_pos:y_pos + height, x_pos:x_pos + width]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def main(current_mode: int,\n",
    "         recognizer: cv2.face.LBPHFaceRecognizer = None) -> None:\n",
    "    \"\"\"\n",
    "    MODE_WAITING:                  Detect faces and draw bounding boxes\n",
    "    MODE_UP_DOWN|MODE_LEFT_RIGHT:  Sample mug shots if single face detected\n",
    "    MODE_AUTHENTICATING:           Detect faces and predict label/confidence\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise the webcam\n",
    "    stream = VideoStream(src=0, framerate=30).start()\n",
    "\n",
    "    # Initialise the classifier for detecting faces\n",
    "    cascade = cv2.CascadeClassifier(CASCADE_PATH)\n",
    "\n",
    "    # Set the frame title by mode\n",
    "    # Initialise a list for the mug shots\n",
    "    if current_mode == MODE_WAITING:\n",
    "        mug_shots = []\n",
    "        frame_title = \"Register New User\"\n",
    "    elif current_mode == MODE_AUTHENTICATING:\n",
    "        frame_title = \"Authenticate User\"\n",
    "\n",
    "    # Keep a list of frames for a gif\n",
    "    all_frames = []\n",
    "\n",
    "    # Whether to save frames to produce a gif\n",
    "    recording = red_circle = False\n",
    "\n",
    "    while True:\n",
    "        # Capture the frame, a gray scale version, and anhy faces in it\n",
    "        frame, gray_frame, faces = capture_frame(stream, cascade)\n",
    "\n",
    "        # Waiting to begin capturing new user images; put a bounding box around all faces\n",
    "        if current_mode == MODE_WAITING:\n",
    "            for (x_pos, y_pos, width, height) in faces:\n",
    "                frame = cv2.rectangle(frame, (x_pos, y_pos), (x_pos + width, y_pos + height), (255, 255, 255), 2)\n",
    "\n",
    "        # In capture mode (LEFT_RIGHT|UP_DOWN) and only 1 face\n",
    "        elif current_mode <= MODE_UP_DOWN and len(faces) == 1:\n",
    "            # Add the mug shot (if it exists) to the list; check for moving to next mode/finishing\n",
    "            mug_shot = get_mug_shot(frame, gray_frame, faces[0])\n",
    "            if mug_shot is not None:\n",
    "                mug_shots.append(mug_shot)\n",
    "                if len(mug_shots) == int(MAX_SHOTS / 2):\n",
    "                    current_mode = MODE_UP_DOWN\n",
    "                elif len(mug_shots) == MAX_SHOTS:\n",
    "                    save_user_images(mug_shots)\n",
    "                    break\n",
    "\n",
    "        # Trying to recognize known users\n",
    "        elif current_mode == MODE_AUTHENTICATING:\n",
    "            for (x_pos, y_pos, width, height) in faces:\n",
    "                # Predict a label and confidence for each face detected\n",
    "                label, confidence = recognizer.predict(gray_frame[y_pos:y_pos + height, x_pos:x_pos + width])\n",
    "\n",
    "                colour = (int(255 / (label + 1)), int(255 / (label + 1)), 255)  # Quick and dirty way to get a different colour per user\n",
    "\n",
    "                # Draw a rectangle around this face with the predicted label/unknown and confidence\n",
    "                frame = cv2.rectangle(frame, (x_pos, y_pos), (x_pos + width, y_pos + height), colour, 2)\n",
    "                cv2.putText(frame, f\"User {label} ({round(confidence, 2)})\", (x_pos + 5, y_pos + height + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, colour, 2)\n",
    "\n",
    "        # Add appropriate message to the frame\n",
    "        add_text(frame, MESSAGES[current_mode])\n",
    "\n",
    "        # If recording is True, add a blinking red circle to the top right and keep the frame\n",
    "        if recording:\n",
    "            # Blinking red circle\n",
    "            if len(all_frames) % 30 == 0:\n",
    "                red_circle = not red_circle\n",
    "            if red_circle:\n",
    "                all_frames.append(np.copy(frame))  # Save a copy without the red circle\n",
    "                cv2.circle(frame, (FRAME_WIDTH - 20, 20), 10, (0, 0, 255), -1)\n",
    "            else:\n",
    "                all_frames.append(frame)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(frame_title, frame)\n",
    "\n",
    "        # Check for a key press\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # User is ready and presses 's'\n",
    "        if current_mode == MODE_WAITING and key == ord(\"s\"):\n",
    "            current_mode = MODE_LEFT_RIGHT\n",
    "\n",
    "        # Save a screen grab of the frame named for the current time\n",
    "        elif key == ord(\"p\"):\n",
    "            name = f\"{datetime.datetime.now().time().strftime('%H_%M_%S')}.jpg\"\n",
    "            cv2.imwrite(os.path.join(os.getcwd(), name), frame)\n",
    "\n",
    "        # Start/stop recording\n",
    "        elif key == ord(\"r\"):\n",
    "            recording = not recording\n",
    "            red_circle = recording\n",
    "\n",
    "        # Quit\n",
    "        elif key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    # If frames have been recorded, save them as a gif\n",
    "    if len(all_frames) > 0:\n",
    "        gif_path = os.path.join(os.getcwd(), \"recording.gif\")\n",
    "        save_gif(all_frames, gif_path)\n",
    "\n",
    "    # Tidy up CV2 and the VideoStream\n",
    "    cv2.destroyAllWindows()\n",
    "    stream.stop()\n",
    "\n",
    "\n",
    "def register_user() -> None:\n",
    "    \"\"\"\n",
    "    Convenience wrapper for adding a user\n",
    "    \"\"\"\n",
    "    main(MODE_WAITING)\n",
    "\n",
    "\n",
    "def authenticate_user(recognizer: cv2.face.LBPHFaceRecognizer) -> None:\n",
    "    \"\"\"\n",
    "    Convenience wrapper for starting facial recognition\n",
    "    \"\"\"\n",
    "    main(MODE_AUTHENTICATING, recognizer=recognizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e18cd1-70dd-4382-952e-35c6a6dee3d5",
   "metadata": {},
   "source": [
    "# Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cee3dd-9311-4969-a068-51bcdf34bb07",
   "metadata": {},
   "source": [
    "## Haar Cascade Classifier\n",
    "<p>\n",
    "    OpenCV provides a pre-trained model, <a href=\"https://github.com/kipr/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml\">haarcascade_frontalface_default.xml</a>, for use with <a href=\"https://docs.opencv.org/4.8.0/db/d28/tutorial_cascade_classifier.html\">cv2.CascadeClassifier</a>.\n",
    "    The Haar Cascade was proposed by Paul Viola and Michael Jones in their 2001 paper <a href=\"https://www.khoury.northeastern.edu/home/vip/teach/DMcourse/3_dim_reduction/notes_slides/viola-cvpr-01.pdf\">Rapid Object Detection using a Boosted Cascade of Simple Features</a>. There's an excellent explanation in <a href=\"https://medium.com/analytics-vidhya/haar-cascades-explained-38210e57970d\">this</a> article, but to summarise, the technique computes <a href=\"https://en.wikipedia.org/wiki/Haar-like_feature\">Haar features</a> over an <a href=\"https://en.wikipedia.org/wiki/Summed-area_table\">integral image</a>, and uses a modified <a href=\"https://en.wikipedia.org/wiki/AdaBoost\">AdaBoost</a> create a series of cascading classifiers. At each stage of the cascade, if a region is determined not to contain an object, it does not progress to the next stage. This is a type of attentional mechanism to make the algotithm focus on the regions most likely to contain a face.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9cd277-5fdf-4a7c-a53b-1abc019647bc",
   "metadata": {},
   "source": [
    "<p>\n",
    "    The <em>register_user()</em> method in the next cell starts the video capture, with an example in this gif. The face detected by the Haar Cascade is surrounded with a bounding box, and mug shots are taken randomly to get a variety of angles.<br/>\n",
    "    <img src=\"register_demo.gif\"/>\n",
    "    \n",
    "</p>\n",
    "<p>\n",
    "    Here are a sample of the captured images which will be used to train the recognizer.\n",
    "    <img src=\"mug_shot_samples.png\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec9af3-2ccc-4fc3-9b4d-ecd3c517cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d44f4-9e22-4459-b8e3-185eca49c711",
   "metadata": {},
   "source": [
    "# Facial Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc39fc5-2437-42d7-a0d4-5ccce552f7af",
   "metadata": {},
   "source": [
    "## Local Binary Pattern Histogram (LBPH)\n",
    "<p>\n",
    "The LBPH face recognizer, implemented in OpenCV as <a href=\"https://docs.opencv.org/4.8.0/df/d25/classcv_1_1face_1_1LBPHFaceRecognizer.html\">cv2.face.LBPHFaceRecognizer</a>, uses a combination of texture analysis and pattern recognition to recognize and differentiate faces.\n",
    "\n",
    "LBPH captures the unique texture of a person's face by breaking down the image into smaller regions and analyzing the patterns within those regions. These patterns, represented as binary values, are then used to construct histograms for each face in a training dataset.\n",
    "\n",
    "When it comes to recognizing a face, LBPH computes the pattern and histogram for a test image and compares it to the patterns and histograms of known faces in the training dataset.\n",
    "\n",
    "LBPH particularly is robust enough to handle variations in lighting, facial expressions, and minor changes in pose.\n",
    "\n",
    "Please check out <a href=\"https://towardsdatascience.com/face-recognition-how-lbph-works-90ec258c3d6b\">this</a> article for a more in-depth explanation.\n",
    "</p>\n",
    "<p>\n",
    "There are a <a href=\"https://docs.opencv.org/4.8.0/df/d25/classcv_1_1face_1_1LBPHFaceRecognizer.html#ac33ba992b16f29f2824761cea5cd5fc5\">few parameters</a> to consider when creating the recognizer, but let's stick with the defaults because they affect performance in fairly predictable ways.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e6428-f41d-40fd-a1e2-173221490b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_recognizer(face_data_path: str = None) -> cv2.face.LBPHFaceRecognizer:\n",
    "    \"\"\"\n",
    "    Return a new LBPHFaceRecognizer.\n",
    "    If a path for data is given, load it\n",
    "    \"\"\"\n",
    "\n",
    "    recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "    if face_data_path:\n",
    "        recognizer.read(face_data_path)\n",
    "    return recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0efe3bb-fe6f-4ab7-9e6c-0756375cbef9",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "<p>\n",
    "    The mug shots saved in the earlier step are loaded and the integer label extracted from the filename.\n",
    "    The <a href=\"https://docs.opencv.org/4.8.0/dd/d65/classcv_1_1face_1_1FaceRecognizer.html#ac8680c2aa9649ad3f55e27761165c0d6\"><i>train()</i></a> method (inherited from <a href=\"https://docs.opencv.org/4.8.0/dd/d65/classcv_1_1face_1_1FaceRecognizer.html\">cv2.face.FaceRecognizer</a>) accepts the mug shots taken earlier and to compute the LBPH and creates a YAML file containing the histograms and the labels.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9640e35-d332-42fb-84f9-28ddc2a1d272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_users() -> (List[np.array], np.array):\n",
    "    \"\"\"\n",
    "    Return the user images and labels\n",
    "    \"\"\"\n",
    "\n",
    "    faces = []\n",
    "    labels = []\n",
    "\n",
    "    for file in sorted(os.listdir(SAVE_PATH)):\n",
    "        match = re.search(PIC_FILE_RE, file)\n",
    "        if match:\n",
    "            # Read in grayscale image and convert to numpy array\n",
    "            img = cv2.imread(os.path.join(SAVE_PATH, file), cv2.IMREAD_GRAYSCALE)\n",
    "            img_numpy = np.array(img, \"uint8\")\n",
    "            faces.append(img_numpy)\n",
    "\n",
    "            # Get the ID of the user from the filename\n",
    "            u_id = int(match.groups()[0])\n",
    "            labels.append(u_id)\n",
    "\n",
    "    return faces, np.array(labels)\n",
    "\n",
    "\n",
    "# Load the user images and labels\n",
    "faces, labels = load_users()\n",
    "\n",
    "# Create and train the recognizer\n",
    "recognizer = create_recognizer()\n",
    "recognizer.train(faces, labels)\n",
    "\n",
    "# Write the recognizer data to file\n",
    "# recognizer.write(FACE_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15135ac5-303e-4fb9-bf1c-df9204b83a97",
   "metadata": {},
   "source": [
    "## Authenticating\n",
    "<p>\n",
    "This step loads the previously computed LBPH and compares them to faces detected in the webcam.\n",
    "</p>\n",
    "\n",
    "![f](recognition_demo.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cb5d50-9af8-4721-be1a-4bb36ecad900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the recognizer with the trained data\n",
    "recognizer = create_recognizer(FACE_DATA)\n",
    "\n",
    "authenticate_user(recognizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eced7e1-4b8c-419e-b2de-c35c825f548c",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a920a-bc66-4343-945f-da05b7254b0d",
   "metadata": {},
   "source": [
    "## Photo of User\n",
    "\n",
    "What's the prediction confidence when persented with a photo of a registered user?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15b3139-1f00-4a93-af50-e820af77d1e4",
   "metadata": {},
   "source": [
    "## Number of neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127673fa-f8a4-4394-adf7-3efdffc07d70",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facialrec",
   "language": "python",
   "name": "facialrec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
